{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import urllib\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from collections import OrderedDict\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[1]\")\n",
    "         .setAppName(\"kddcup.data.gz local[1]\")\n",
    "         .set(\"spark.executor.memory\", \"512M\")\n",
    "         .set(\"spark.cores.max\", 4))\n",
    "# sc.stop()\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kddcup_data = sc.textFile(\"file:///home/hduser/sandbox/kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    clean_line_split = [line_split[0]]+line_split[4:-1]\n",
    "    return (line_split[-1], array([float(x) for x in clean_line_split]))\n",
    "\n",
    "parsed_data = kddcup_data.map(parse_interaction)\n",
    "pd_values = parsed_data.values().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489191"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sample = pd_values.sample(False, .10, 123)\n",
    "kdd_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler(True, True)\n",
    "standardizer_model = standardizer.fit(kdd_sample)\n",
    "data_for_cluster = standardizer_model.transform(kdd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(10,31,10)\n",
    "metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(point): \n",
    "    center = clusters.centers[clusters.predict(point)] \n",
    "    denseCenter = DenseVector(numpy.ndarray.tolist(center)) \n",
    "    return sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10, 694538.8491664116), (20, 414102.2721093745), (30, 352680.3941882866)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fails because it cannot find file in other nodes. \n",
    "#What if i try using local[1]?\n",
    "for k in k_values:\n",
    "     clusters = KMeans.train(data_for_cluster, k, maxIterations=4, runs=5, initializationMode=\"random\")\n",
    "     WSSSE = data_for_cluster.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "     results = (k,WSSSE)\n",
    "     metrics.append(results)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 329911.20874727593\n",
      "1722.3157720565796\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "clusters = KMeans.train(data_for_cluster, 30, maxIterations=4, runs=5, initializationMode=\"random\")\n",
    "WSSSE = data_for_cluster.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(30, WSSSE)\n",
    "\n",
    "tt = time.time() - t0\n",
    "\n",
    "\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark Python 3",
   "language": "python3",
   "name": "pyspark-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
