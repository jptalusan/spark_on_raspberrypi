{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This demonstrates how to run spark locally</b><br>\n",
    "When using the local master, you can read files locally.\n",
    "In this case, version7.txt is local, it will not be copied to other nodes.\n",
    "\n",
    "You can verify this by running\n",
    "<code>hdfs dfs -ls</code>\n",
    "on other nodes.<br>\n",
    "\n",
    "Reference:<a href=\"https://spark.apache.org/docs/latest/configuration.html\">Configuration.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the number of threads that are assigned to this by:<br>\n",
    "<code>\n",
    "val conf = new SparkConf()\n",
    "             .setMaster(\"local[2]\")\n",
    "             .setAppName(\"CountingSheep\")\n",
    "val sc = new SparkContext(conf)\n",
    "</code><br>\n",
    "\n",
    "Without these settings, the following is set in the kernel.json of Pyspark Python 3<br>\n",
    "<code>\n",
    "setMaster(\"spark://node1:7077\")\n",
    "</code><br>\n",
    "This means that the setup would use client mode and will use the rest of the clusters as well as the hdfs. the sc.textFile(...), would need to use hdfs:/// or if only filename is placed in, it will use hdfs by default.\n",
    "<br>\n",
    "\n",
    "Try using this?\n",
    "<code>\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"spark://node1:7077\")\n",
    "         .setAppName(\"My app\")\n",
    "         .set(\"spark.executor.memory\", \"512M\")\n",
    "         .set(\"spark.cores.max\", 4)\n",
    "         .set(\"spark.submit.deployMode\", \"client\"))\n",
    "</code>\n",
    "\n",
    "Reference: <a href=\"https://spark.apache.org/docs/latest/submitting-applications.html\">This</a>\n",
    "\n",
    "<p>Some of the commonly used options are:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><code>--class</code>: The entry point for your application (e.g. <code>org.apache.spark.examples.SparkPi</code>)</li>\n",
    "  <li><code>--master</code>: The <a href=\"#master-urls\">master URL</a> for the cluster (e.g. <code>spark://23.195.26.187:7077</code>)</li>\n",
    "  <li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>) (default: <code>client</code>) <b> &#8224; </b></li>\n",
    "  <li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap &#8220;key=value&#8221; in quotes (as shown).</li>\n",
    "  <li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an <code>hdfs://</code> path or a <code>file://</code> path that is present on all nodes.</li>\n",
    "  <li><code>application-arguments</code>: Arguments passed to the main method of your main class, if any</li>\n",
    "</ul>\n",
    "\n",
    "Right now, using jupyter, <a href=\"https://stackoverflow.com/questions/45997150/can-i-run-a-pyspark-jupyter-notebook-in-cluster-deploy-mode\">we only use client, because cluster is not supported</a>. But it should(?) be the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[2]\")\n",
    "         .setAppName(\"My app \")\n",
    "         .set(\"spark.executor.memory\", \"512M\")\n",
    "         .set(\"spark.cores.max\", 4))\n",
    "# sc.stop()\n",
    "sc = SparkContext(conf = conf)\n",
    "local_file = sc.textFile(\"version7.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = sc.textFile(\"file:///home/hduser/sandbox/version7.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = local_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[8] at RDD at PythonRDD.scala:48\n",
      "[('laboris', 1), ('wirc', 1), ('mmodo', 1), ('', 1), ('anim', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "print(counts.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using cluster/client, (not local), file system will be hdfs, and this means that it will be duplicated to all nodes(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts.saveAsTextFile(\"file:///home/hduser/sandbox/version7out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, set eiusmod tempor incidunt et labore et dolore magna aliquam. Ut enim ad minim veniam, quis nostrud exerc. Irure dolor in reprehend incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse molestaie cillum. Tia non ob ea soluad incommod quae egen ium improb fugiend. Officia deserunt mollit anim id est laborum Et harumd dereud facilis est er expedit distinct. Nam liber te conscient to factor tum poen legum odioque civiuda et tam. Neque pecun modut est neque nonor et imper ned libidig met, consectetur adipiscing elit, sed ut labore et dolore magna aliquam is nostrud exercitation ullam mmodo consequet. Duis aute in voluptate velit esse cillum dolore eu fugiat nulla pariatur. At vver eos et accusam dignissum qui blandit est praesent. Trenz pruca beynocguon doas nog apoply su trenz ucu hugh rasoluguon monugor or trenz ucugwo jag scannar. Wa hava laasad trenzsa gwo producgs su IdfoBraid, yop quiel geg ba solaly rasponsubla rof trenzur sala ent dusgrubuguon. Offoctivo immoriatoly, hawrgasi pwicos asi sirucor. Thas sirutciun applios tyu thuso itoms ghuso pwicos gosi sirucor in mixent gosi sirucor ic mixent ples cak ontisi sowios uf Zerm hawr rwivos. Unte af phen neige pheings atoot Prexs eis phat eit sakem eit vory gast te Plok peish ba useing phen roxas. Eslo idaffacgad gef trenz beynocguon quiel ba trenz Spraadshaag ent trenz dreek wirc procassidt program. Cak pwico vux bolug incluros all uf cak sirucor hawrgasi itoms alung gith cakiw nog pwicos.\n",
      "\n",
      "Plloaso mako nuto uf cakso dodtos anr koop a cupy uf cak vux noaw yerw phuno. Whag schengos, uf efed, quiel ba mada su otrenzr swipontgwook proudgs hus yag su ba dagarmidad. Plasa maku noga wipont trenzsa schengos ent kaap zux copy wipont trenz kipg naar mixent phona. Cak pwico siructiun ruos nust apoply tyu cak UCU sisulutiun munityuw uw cak UCU-TGU jot scannow. Trens roxas eis ti Plokeing quert loppe eis yop prexs. Piy opher hawers, eit yaggles orn ti sumbloat alohe plok. Su havo loasor cakso tgu pwuructs tyu InfuBwain, ghu gill nug bo suloly sispunsiblo fuw cakiw salo anr ristwibutiun. Hei muk neme eis loppe. Treas em wankeing ont sime ploked peish rof phen sumbloat syug si phat phey gavet peish ta paat ein pheeir sumbloats. Aslu unaffoctor gef cak siructiun gill bo cak spiarshoot anet cak GurGanglo gur pwucossing pwutwam. Ghat dodtos, ig pany, gill bo maro tyu ucakw suftgasi pwuructs hod yot tyubo rotowminor. Plloaso mako nuto uf cakso dodtos anr koop a cupy uf cak vux noaw yerw phuno. Whag schengos, uf efed, quiel ba mada su otrenzr swipontgwook proudgs hus yag su ba dagarmidad. Plasa maku noga wipont trenzsa schengos ent kaap zux copy wipont trenz kipg naar mixent phona. Cak pwico siructiun ruos nust apoply tyu cak UCU sisulutiun munityuw uw cak UCU-TGU jot scannow. Trens roxas eis ti Plokeing quert loppe eis yop prexs. Piy opher hawers, eit yaggles orn ti sumbloat alohe plok. Su havo loasor cakso tgu pwuructs tyu.\n"
     ]
    }
   ],
   "source": [
    "with open(\"version7.txt\", 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "    - https://arnesund.com/2015/09/21/spark-cluster-on-openstack-with-multi-user-jupyter-notebook/\n",
    "    - Differences:\n",
    "    https://community.hortonworks.com/questions/89263/difference-between-local-vs-yarn-cluster-vs-yarn-c.html\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = sc.textFile(\"file:///home/hduser/sandbox/sample_lda_data.txt\").map(lambda row: row.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:0.27293306589126587\n",
      "3:0.2072264403104782\n",
      "0:0.13399849832057953\n",
      "1:0.0637863352894783\n",
      "9:0.04702875018119812\n"
     ]
    }
   ],
   "source": [
    "synonyms = model.findSynonyms('2', 5)\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}:{}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark Python 3",
   "language": "python3",
   "name": "pyspark-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
