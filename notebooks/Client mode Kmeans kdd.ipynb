{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare process time when files are all distributed compared to just in 1 node?\n",
    "#Same as untitled but will distribute the file to all nodes first and then executeimport findspark\n",
    "#https://mapr.com/blog/tutorial-using-pyspark-and-mapr-sandbox/\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import urllib\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from collections import OrderedDict\n",
    "from time import time\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName('Sparkl Python RDD Basics')\n",
    "conf.set(\"spark.cores.max\", \"16\")\n",
    "conf.set(\"spark.yarn.executor.memoryOverhead\", \"0\")\n",
    "conf.set(\"spark.yarn.executor.memory\", \"512M\")\n",
    "conf.set(\"spark.yarn.driver.memory\", \"512M\")\n",
    "conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "# sc.stop()\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if file is in hduser, no need for file:///\n",
    "data_file = \"kddcup.data.gz\"\n",
    "kddcup_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    clean_line_split = [line_split[0]]+line_split[4:-1]\n",
    "    return (line_split[-1], array([float(x) for x in clean_line_split]))\n",
    "\n",
    "parsed_data = kddcup_data.map(parse_interaction)\n",
    "pd_values = parsed_data.values().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_sample = pd_values.sample(False, .10, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler(True, True)\n",
    "standardizer_model = standardizer.fit(kdd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_cluster = standardizer_model.transform(kdd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(10, 31, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values = range(10,31,10)\n",
    "metrics = []\n",
    "\n",
    "k_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(point): \n",
    "    center = clusters.centers[clusters.predict(point)] \n",
    "    denseCenter = DenseVector(numpy.ndarray.tolist(center)) \n",
    "    return sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 306384.7358574851\n",
      "3299.7056498527527\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "clusters = KMeans.train(data_for_cluster, 30, maxIterations=4, initializationMode=\"k-means||\")\n",
    "WSSSE = data_for_cluster.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(30, WSSSE)\n",
    "\n",
    "tt = time.time() - t0\n",
    "\n",
    "\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark Python 3",
   "language": "python3",
   "name": "pyspark-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
